{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading and Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the item and sales Data\n",
    "items_df = pd.read_csv(r\"C:\\Users\\09sra\\Downloads\\Data Science and Engineering Assignment Datasets\\Data Sciene Internship Assignment Datasets\\item.csv\")\n",
    "sales_df = pd.read_csv(r\"C:\\Users\\09sra\\Downloads\\Data Science and Engineering Assignment Datasets\\Data Sciene Internship Assignment Datasets\\sales.csv\")\n",
    "\n",
    "# Merge sales and items datasets on the 'code' column\n",
    "merged_df = pd.merge(sales_df, items_df, on='code', how='inner')\n",
    "\n",
    "# Create a binary feature for voucher usage\n",
    "merged_df['voucher_used'] = (merged_df['voucher'] > 0).astype(int)\n",
    "\n",
    "# Create a day-of-week feature from 'day'\n",
    "merged_df['day_of_week'] = merged_df['day'] % 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering and Model Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns relevant for modeling\n",
    "model_df = merged_df[['units', 'voucher_used', 'type', 'brand', 'size', 'day_of_week']].copy()\n",
    "\n",
    "# Drop any rows with NaN\n",
    "model_df.dropna(subset=['units','voucher_used','type','brand','size','day_of_week'], inplace=True)\n",
    "\n",
    "# Our target is 'units'\n",
    "y = model_df['units']\n",
    "\n",
    "# creating a Feature matrix by dropping 'units' column\n",
    "X = model_df.drop('units', axis=1)\n",
    "\n",
    "# One-hot encoding for  categorical features such as type, brand, size, day_of_week\n",
    "X = pd.get_dummies(X, columns=['type','brand','size','day_of_week'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.041\n",
      "Test R^2:  0.032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test(80% training data and 20% testing data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train the RandomForestRegressor model\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating to check model performance using RÂ² score\n",
    "r2_score_train = model.score(X_train, y_train)\n",
    "r2_score_test  = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train R^2: {r2_score_train:.3f}\")\n",
    "print(f\"Test R^2:  {r2_score_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uplift Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Voucher Allocation (Predicted Uplift in Units):\n",
      "Type 2: Uplift = 0.18\n",
      "Type 3: Uplift = 0.07\n",
      "Type 1: Uplift = 0.07\n",
      "Type 4: Uplift = 0.07\n"
     ]
    }
   ],
   "source": [
    "item_types = merged_df['type'].unique()\n",
    "recommendations = []\n",
    "\n",
    "# calculates predicted uplift in units sold when voucher is applied\n",
    "for item_type in item_types:\n",
    "    # Filter rows for this particular item_type\n",
    "    subset = merged_df[merged_df['type'] == item_type].copy()\n",
    "    \n",
    "    if subset.empty:\n",
    "        continue  \n",
    "\n",
    "    # build a dummy-encoded version of the relevant features\n",
    "    subset_features = subset[['voucher_used', 'type', 'brand', 'size', 'day_of_week']].copy()\n",
    "    subset_features = pd.get_dummies(\n",
    "        subset_features, \n",
    "        columns=['type','brand','size','day_of_week'],\n",
    "        drop_first=True\n",
    "    )\n",
    "    \n",
    "    # Make sure we align columns with the model's X\n",
    "    all_needed_cols = set(X.columns)  # columns used in training\n",
    "    subset_cols = set(subset_features.columns)\n",
    "    missing_cols = all_needed_cols - subset_cols\n",
    "    \n",
    "    # Add missing columns as 0 in one go to avoid fragmentation\n",
    "    # This ensures the same dimensionality as X\n",
    "    if missing_cols:\n",
    "        # create a zero-filled DataFrame for missing cols\n",
    "        zeros_df = pd.DataFrame(0, index=subset_features.index, columns=list(missing_cols))\n",
    "        # concatenate\n",
    "        subset_features = pd.concat([subset_features, zeros_df], axis=1)\n",
    "    \n",
    "    # Reorder to match X exactly\n",
    "    subset_features = subset_features[X.columns]\n",
    "\n",
    "    # Compute mean row for that item type\n",
    "    mean_vector = subset_features.mean()\n",
    "\n",
    "    # Create two copies, no-voucher and voucher\n",
    "    mean_vector_no_voucher = mean_vector.copy()\n",
    "    mean_vector_no_voucher['voucher_used'] = 0\n",
    "    \n",
    "    mean_vector_voucher = mean_vector.copy()\n",
    "    mean_vector_voucher['voucher_used'] = 1\n",
    "\n",
    "    # Convert these Series to DataFrame rows\n",
    "    df_no_voucher = pd.DataFrame([mean_vector_no_voucher], columns=X.columns)\n",
    "    df_voucher    = pd.DataFrame([mean_vector_voucher], columns=X.columns)\n",
    "\n",
    "    # Predict the no of units sold with and without voucher\n",
    "    pred_no_voucher = model.predict(df_no_voucher)[0]\n",
    "    pred_voucher    = model.predict(df_voucher)[0]\n",
    "\n",
    "    # Uplift = difference\n",
    "    uplift = pred_voucher - pred_no_voucher\n",
    "    recommendations.append((item_type, uplift))\n",
    "\n",
    "# Sort by descending uplift\n",
    "recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output recommended Voucher Allocation\n",
    "print(\"Recommended Voucher Allocation (Predicted Uplift in Units):\")\n",
    "for item_type, uplift_val in recommendations:\n",
    "    print(f\"{item_type}: Uplift = {uplift_val:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
